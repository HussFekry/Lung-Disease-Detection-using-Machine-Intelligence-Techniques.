{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOLGMcU-XaTR"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_files(base_dir, target_dir):\n",
        "    count = 0\n",
        "    path = get_path(base_dir, target_dir)\n",
        "    for dirname, _, filenames in os.walk(path):\n",
        "        for filename in filenames:\n",
        "            count+=len(glob.glob(os.path.join(dirname, filename)))\n",
        "        return path, count\n",
        "\n",
        "def get_path(base_dir, target_dir):\n",
        "    path = os.path.join(base_dir,target_dir)\n",
        "    return path"
      ],
      "metadata": {
        "id": "cAuuzNz4XdUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = 'D:/Machine Proj/Project/Dataset'\n",
        "\n",
        "train_normal_dir = 'D:\\Data\\chest_xray/train/normal'\n",
        "train_pneumonia_dir = 'D:\\Data\\chest_xray/train/opacity'\n",
        "\n",
        "val_normal_dir = 'D:\\Data\\chest_xray/val/normal'\n",
        "val_pneumonia_dir = 'D:\\Data\\chest_xray/val/opacity'\n",
        "\n",
        "test_normal_dir = 'D:\\Data\\chest_xray/test/normal'\n",
        "test_pneumonia_dir = 'D:\\Data\\chest_xray/test/opacity'\n",
        "\n",
        "\n",
        "train_normal_path, train_normal_count = get_files(base_dir,train_normal_dir)\n",
        "train_pneumonia_path, train_pneumonia_count = get_files(base_dir,train_pneumonia_dir)\n",
        "\n",
        "val_normal_path, val_normal_count = get_files(base_dir,val_normal_dir)\n",
        "val_pneumonia_path, val_pneumonia_count = get_files(base_dir,val_pneumonia_dir)\n",
        "\n",
        "test_normal_path, test_normal_count = get_files(base_dir,test_normal_dir)\n",
        "test_pneumonia_path, test_pneumonia_count = get_files(base_dir,test_pneumonia_dir)\n",
        "\n",
        "print(\"No of Train Images: {}\".format(train_normal_count + train_pneumonia_count))\n",
        "print(\" \\u2022 No of Normal Images {}\".format(train_normal_count))\n",
        "print(\" \\u2022 No of Pneumonia Images {}\".format(train_pneumonia_count))\n",
        "\n",
        "print(\"No of Validation Images: {}\".format(val_normal_count + val_pneumonia_count))\n",
        "print(\" \\u2022 No of Normal Images {}\".format(val_normal_count))\n",
        "print(\" \\u2022 No of Pneumonia Images {}\".format(val_pneumonia_count))\n",
        "\n",
        "print(\"No of Test Images: {}\".format(test_normal_count + test_pneumonia_count))\n",
        "print(\" \\u2022 No of Normal Images {}\".format(test_normal_count))\n",
        "print(\" \\u2022 No of Pneumonia Images {}\".format(test_pneumonia_count))"
      ],
      "metadata": {
        "id": "wU7Jl5v4XdXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "for filename in os.listdir(train_normal_path):\n",
        "    train_data.append((os.path.join(train_normal_path,filename),0))\n",
        "\n",
        "for filename in os.listdir(train_pneumonia_path):\n",
        "    train_data.append((os.path.join(train_pneumonia_path,filename),1))\n",
        "\n",
        "train_data = pd.DataFrame(train_data, columns=['image_path', 'label'], index=None)\n",
        "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
        "        \n",
        "val_data = []\n",
        "for filename in os.listdir(val_normal_path):\n",
        "    val_data.append((os.path.join(val_normal_path,filename),0))\n",
        "\n",
        "for filename in os.listdir(val_pneumonia_path):\n",
        "    val_data.append((os.path.join(val_pneumonia_path,filename),1))\n",
        "        \n",
        "val_data = pd.DataFrame(val_data, columns=['image_path', 'label'], index=None)\n",
        "        \n",
        "test_data = []\n",
        "for filename in os.listdir(test_normal_path):\n",
        "    test_data.append((os.path.join(test_normal_path,filename),0))\n",
        "\n",
        "for filename in os.listdir(test_pneumonia_path):\n",
        "    test_data.append((os.path.join(test_pneumonia_path,filename),1))\n",
        "\n",
        "test_data = pd.DataFrame(test_data, columns=['image_path', 'label'], index=None)\n",
        "\n",
        "print(\"Train Data {}\".format(train_data.shape))\n",
        "print(\"Validation Data {}\".format(val_data.shape))\n",
        "print(\"Test Data {}\".format(test_data.shape))"
      ],
      "metadata": {
        "id": "X70sN5_KXddO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data\n"
      ],
      "metadata": {
        "id": "CTdRPLBAXdg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_dict = {0:'Normal', 1:'Pneumonia'}\n",
        "train_data['class_name'] = train_data.label.map(class_dict)\n",
        "train_data['class_name'].value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "Vr2CW-7jXdj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filepath in train_data.image_path:\n",
        "    image = cv2.imread(filepath)\n",
        "    image_size = image.shape\n",
        "    break\n",
        "image_size"
      ],
      "metadata": {
        "id": "Z3fqYfilXdmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "IMG_SHAPE  = 224\n",
        "\n",
        "train_image_gen = ImageDataGenerator(rescale=1./255,\n",
        "                                     width_shift_range=0.1,\n",
        "                                     height_shift_range=0.1,\n",
        "                                     brightness_range=[0.2,1.0],\n",
        "                                     zoom_range=0.2,\n",
        "                                     horizontal_flip=True,\n",
        "                                     fill_mode='nearest')\n",
        "\n",
        "train_gen = train_image_gen.flow_from_dataframe(train_data,\n",
        "                                              x_col='image_path',\n",
        "                                              y_col='class_name',\n",
        "                                              class_mode='binary',\n",
        "                                              batch_size=BATCH_SIZE,\n",
        "                                              shuffle=True,\n",
        "                                              target_size=(IMG_SHAPE,IMG_SHAPE))"
      ],
      "metadata": {
        "id": "WxaCC1jKXdpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_images = [train_gen[0][0][2] for i in range(5)]\n",
        "plotImages(augmented_images)"
      ],
      "metadata": {
        "id": "MClObBrdXdyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "train_lb = to_categorical(train_data.label, dtype = int)\n",
        "val_lb = to_categorical(val_data.label, dtype=int)\n",
        "\n",
        "train_data = train_data.reset_index().drop(labels='index', axis=1)\n",
        "y_train = pd.DataFrame(train_lb).add_prefix('label_')\n",
        "\n",
        "val_data = val_data.reset_index().drop(labels='index', axis=1)\n",
        "y_val = pd.DataFrame(val_lb).add_prefix('label_')\n",
        "\n",
        "train_data = pd.concat([train_data, y_train], axis=1)\n",
        "val_data = pd.concat([val_data, y_val], axis=1)\n",
        "\n",
        "print(\"Training set has {} samples\".format(train_data.shape[0]))\n",
        "print(\"Validation set has {} samples\".format(val_data.shape[0]))"
      ],
      "metadata": {
        "id": "2LwF0SvbXd1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "IMG_SHAPE  = 224\n",
        "EPOCHS = 20\n",
        "\n",
        "def gen():\n",
        "    train_image_gen = ImageDataGenerator(rescale=1./255,\n",
        "                                         width_shift_range=0.1,\n",
        "                                         height_shift_range=0.1,\n",
        "                                         brightness_range=[0.2,1.0],\n",
        "                                         zoom_range=0.2,\n",
        "                                         horizontal_flip=True,\n",
        "                                         vertical_flip=True,\n",
        "                                         fill_mode='nearest')\n",
        "\n",
        "    train_gen = train_image_gen.flow_from_dataframe(train_data,\n",
        "                                              x_col='image_path',\n",
        "                                              y_col=[f'label_{x}' for x in np.arange(2)],\n",
        "                                              class_mode='raw',\n",
        "                                              batch_size=BATCH_SIZE,\n",
        "                                              shuffle=True,\n",
        "                                              target_size=(IMG_SHAPE,IMG_SHAPE))\n",
        "\n",
        "\n",
        "    val_image_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    val_gen = val_image_gen.flow_from_dataframe(val_data,\n",
        "                                              x_col='image_path',\n",
        "                                              y_col= [f'label_{x}' for x in np.arange(2)],\n",
        "                                              class_mode='raw',\n",
        "                                              batch_size=BATCH_SIZE,\n",
        "                                              target_size=(IMG_SHAPE,IMG_SHAPE))\n",
        "    return train_gen, val_gen"
      ],
      "metadata": {
        "id": "EKCzOdYMXd3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(history):\n",
        "\n",
        "    training_accuracy = history.history['accuracy']\n",
        "    validation_accuracy = history.history['val_accuracy']\n",
        "\n",
        "    training_loss = history.history['loss']\n",
        "    validation_loss = history.history['val_loss']\n",
        "\n",
        "    epochs_range=range(len(training_accuracy))\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, training_accuracy, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, validation_accuracy, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, training_loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, validation_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bGDtO6Y1XeEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "def predict(image_path, model):\n",
        "    im = cv2.imread(image_path)\n",
        "    test_image = np.asarray(im)\n",
        "    processed_test_image = process_image(test_image)\n",
        "    processed_test_image = np.expand_dims(processed_test_image, axis = 0)\n",
        "    \n",
        "    ps = model.predict(processed_test_image)\n",
        "    return ps\n",
        "    \n",
        "def process_image(image):\n",
        "    image = tf.cast(image , tf.float32)\n",
        "    image = tf.image.resize(image , (224 , 224))\n",
        "    image = image/255\n",
        "    image = image.numpy()\n",
        "    return image"
      ],
      "metadata": {
        "id": "AyMDsnMgZEkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "\n",
        "base = MobileNetV2(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\n",
        "tf.keras.backend.clear_session()\n",
        "    \n",
        "for layer in base.layers:\n",
        "    layer.trainable =  False\n",
        "\n",
        "mobilenet_model = Sequential()\n",
        "mobilenet_model.add(base)\n",
        "mobilenet_model.add(GlobalAveragePooling2D())\n",
        "mobilenet_model.add(BatchNormalization())\n",
        "mobilenet_model.add(Dense(256, activation='relu'))\n",
        "mobilenet_model.add(Dropout(0.5))\n",
        "mobilenet_model.add(BatchNormalization())\n",
        "mobilenet_model.add(Dense(128, activation='relu'))\n",
        "mobilenet_model.add(Dropout(0.5))\n",
        "mobilenet_model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "mobilenet_model.summary()"
      ],
      "metadata": {
        "id": "eQPTU37KZEm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen, val_gen = gen()\n",
        "\n",
        "optm = Adam(lr=0.0001)\n",
        "mobilenet_model.compile(loss='binary_crossentropy', optimizer=optm, \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "EarlyStopping = EarlyStopping(monitor='val_loss',\n",
        "                              min_delta=.0001,\n",
        "                              patience=3,\n",
        "                              verbose=1,\n",
        "                              mode='auto',\n",
        "                              restore_best_weights=True)\n",
        "\n",
        "model_save = ModelCheckpoint('./mobilenetV2.h5',\n",
        "                             save_best_only = True,\n",
        "                             save_weights_only = False,\n",
        "                             monitor = 'val_loss', \n",
        "                             mode = 'min', verbose = 1)\n",
        "\n",
        "\n",
        "mob_history = mobilenet_model.fit(train_gen,\n",
        "                              steps_per_epoch = train_gen.samples // BATCH_SIZE,\n",
        "                              epochs = EPOCHS,\n",
        "                              validation_data = val_gen,\n",
        "                              callbacks=[EarlyStopping, model_save])"
      ],
      "metadata": {
        "id": "H92fydL3ZEtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(mob_history)\n"
      ],
      "metadata": {
        "id": "ko-uqJF5ZS2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mob_pred =[]\n",
        "for image in test_data.image_path:\n",
        "    mob_pred.append(predict(image , mobilenet_model))\n",
        "    \n",
        "final_mob_pred  = np.argmax(mob_pred , axis=-1)\n",
        "actual_label = test_data['label']\n",
        "\n",
        "print(classification_report(actual_label, final_mob_pred))\n",
        "matrix=confusion_matrix(actual_label, final_mob_pred)\n",
        "sns.heatmap(matrix,square=True, annot=True, fmt='d', cbar=False,\n",
        "            xticklabels=['0', '1'],\n",
        "            yticklabels=['0', '1'])\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label');"
      ],
      "metadata": {
        "id": "wGGuYiw5ZS9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mob_history.history['val_accuracy'][-4])\n",
        "print(mob_history.history['val_loss'][-4])"
      ],
      "metadata": {
        "id": "c8J-p0b0ZTMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HOlLhvwnZTT_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}